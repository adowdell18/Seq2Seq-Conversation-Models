{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0JzHImYOXhiv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Coments and code adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "owLNblxnVoe7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "   #import tensorflow as tf\n",
        "from tensorflow.python.layers.core import Dense\n",
        "\n",
        "#assert tf.__version__ == '1.4.0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLh7G9okTLGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import config\n",
        "#from model_utils import Chatbot\n",
        "#from cornell_data_utils import *\n",
        "from tqdm import tqdm\n",
        "import codecs\n",
        "import math\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EI_zu3o6z7c8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "fAG0_rxEWNfG",
        "colab_type": "code",
        "outputId": "e48a8202-0d09-4ad5-d24a-51d7a79586bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "#import config\n",
        "from collections import Counter\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D2MbX8pHZA2_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "#model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjasBXMBz73X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Dd7o4PdRnMU_",
        "colab_type": "code",
        "outputId": "fe179bf7-d293-4c64-ed91-e191e4c80833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JvU6h7F7WTmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def grap_inputs():\n",
        "    #Following comments Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "\n",
        "    '''\n",
        "\t\tThis function is used to define all tensorflow graph placeholders (inputs to the TF graph)\n",
        "\n",
        "\t\tInputs: None\n",
        "\n",
        "\t\tOutputs:\n",
        "\t\t\tinputs - questions in the case of a Chatbot with dimensions of None, None = batch_size, questions_length\n",
        "\t\t\ttargets - answers in the case of a Chatbot with dimensions of None, None = batch_size, answers_length\n",
        "\t\t\tkeep_probs - probabilities used in dropout layer\n",
        "\n",
        "\t\t\tencoder_seq_len -  vector which is used to define lenghts of each sample in the inputs to the model\n",
        "\t\t\tdecoder_seq_len - vector which is used to define lengths of each sample in the targets to the model\n",
        "\t\t\tmax_seq_len - target sample with the most words in it\n",
        "\n",
        "    '''\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    keep_probs = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "    \n",
        "    encoder_seq_len = tf.placeholder(tf.int32, (None, ), name='encoder_seq_len')\n",
        "    decoder_seq_len = tf.placeholder(tf.int32, (None, ), name='decoder_seq_len')\n",
        "    \n",
        "    #encoder_seq_len = tf.placeholder(tf.int32, ([]), name='encoder_seq_len')\n",
        "    #decoder_seq_len = tf.placeholder(tf.int32, ([]), name='decoder_seq_len')\n",
        "    \n",
        "    max_seq_len = tf.reduce_max(decoder_seq_len, name='max_seq_len')\n",
        "    \n",
        "    return inputs, targets, keep_probs, encoder_seq_len, decoder_seq_len, max_seq_len\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def encoders(inputs, rnn_size, number_of_layers, encoder_seq_len, keep_probs, encoder_embed_size, encoder_vocab_size):\n",
        "\n",
        "\t   #Following comments Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "\n",
        "\t\t#Used to define encoder of the seq2seq model (The encoder is made of simple dynamic RNN network).\n",
        "\n",
        "\t\t#Inputs:\n",
        "\t\t\t#inputs -\n",
        "\t\t\t#rnn_siz - number of units in the RNN layer\n",
        "\t\t\t#number_of_layer - number of RNN layers that the model uses\n",
        "\t\t\t#encoder_seq_len - vector of lengths (got from placeholder)\n",
        "\t\t\t#keep_probs - dropout rate\n",
        "\t\t\t#encoder_embed_size - size of embedding vector for encoder part\n",
        "\t\t\t#encoder_vocab_size - number of different words that the model uses in a vocabulary\n",
        "\t\t\n",
        "\t\t#Outputs:\n",
        "\t\t\t#encoder_outputs -\n",
        "\t\t\t#encoder_states - internal states from the RNN layer(s)\n",
        "    \n",
        "    \n",
        "    \n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([cell(rnn_size, keep_probs) for _ in range(number_of_layers)])\n",
        "     \n",
        "    encoder_embedings = tf.contrib.layers.embed_sequence(inputs, encoder_vocab_size, encoder_embed_size) #used to create embeding layer for the encoder\n",
        "    \n",
        "    encoder_outputs, encoder_states = tf.nn.dynamic_rnn(encoder_cell, \n",
        "                                                        encoder_embedings, \n",
        "                                                        encoder_seq_len, \n",
        "                                                        dtype=tf.float32)\n",
        "    \n",
        "    return encoder_outputs, encoder_states\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def cell(units, rate):\n",
        "        layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
        "        return tf.contrib.rnn.DropoutWrapper(layer, rate)\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def decoder_inputs_preprocessing(targets, word_to_id, batch_size):\n",
        "\t\n",
        "\t\t#Helper function used to prepare decoder inputs\n",
        "\n",
        "\t\t#Inputs:\n",
        "\t\t\t#targets -\n",
        "\t\t\t#word_to_id - dictionery that the model uses to map each word to it's int representation\n",
        "\t\t\t#batch_size - number of samples that we put through the model at onces\n",
        "\n",
        "\t\t#Outputs:\n",
        "\t\t\t#preprocessed version of decoder inputs\n",
        "\n",
        "    endings = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1]) #This line is used to REMOVE last member of each sample in the decoder_inputs batch\n",
        "    return tf.concat([tf.fill([batch_size, 1], word_to_id['<GO>']), endings], 1) #returning line and in this line we concat '<GO>' tag at the beginning of each sample in the batch\n",
        "\n",
        "  #Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def decoders(decoder_inputs, enc_states, dec_cell, decoder_embed_size, vocab_size,\n",
        "            dec_seq_len, max_seq_len, word_to_id, batch_size):\n",
        "\n",
        "\t\n",
        "\t\t#Following comments Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "\n",
        "\t\t#The decoder core function.\n",
        "\n",
        "\t\t#Inputs:\n",
        "\t\t\t#decoder_inputs -\n",
        "\t\t\t#enc_states - states created by the encoder part of the seq2seq network\n",
        "\t\t\t#dec_cell - RNN cell used in the decoder RNN (can be attention cell as well)\n",
        "\t\t\t#decoder_embed_size - vector size of the decoder embedding layer\n",
        "\t\t\t#vocab_size - number of different words used in the decoder part\n",
        "\t\t\t#dec_seq_len - vector of lengths for the decoder, obtained from the placeholder\n",
        "\t\t\t#max_seq_len - sample with max number of words (got from placeholder)\n",
        "\t\t\t#word_to_id - python dict used to encode each word to it's int representation\n",
        "\t\t\t#batch_size - number of samples that we put through the model at onces\n",
        "\n",
        "\t\t#Outputs:\n",
        "\t\t\t#train_dec_outputs -\n",
        "\t\t\t#inference_dec_output - Inportant for testing and production use!\n",
        "\t\n",
        "    \n",
        "    #Defining embedding layer for the Decoder\n",
        "    embed_layer = tf.Variable(tf.random_uniform([vocab_size, decoder_embed_size]))\n",
        "    embedings = tf.nn.embedding_lookup(embed_layer, decoder_inputs) \n",
        "    \n",
        "    #Creating Dense (Fully Connected) layer at the end of the Decoder -  used for generating probabilities for each word in the vocabulary\n",
        "    output_layer = Dense(vocab_size, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.1))\n",
        "    \n",
        "\n",
        "    with tf.variable_scope('decoder'):\n",
        "        #Training helper used only to read inputs in the TRAINING stage\n",
        "        train_helper = tf.contrib.seq2seq.TrainingHelper(embedings, \n",
        "                                                          dec_seq_len)\n",
        "        \n",
        "        #Defining decoder - You can change with BeamSearchDecoder, just beam size\n",
        "        train_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                                        train_helper, \n",
        "                                                        enc_states, \n",
        "                                                        output_layer)\n",
        "        \n",
        "        #Finishing the training decoder\n",
        "        train_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, \n",
        "                                                                    impute_finished=True, \n",
        "                                                                    maximum_iterations=max_seq_len)\n",
        "        \n",
        "    with tf.variable_scope('decoder', reuse=True): #we use REUSE option in this scope because we want to get same params learned in the previouse 'decoder' scope\n",
        "        #getting vector of the '<GO>' tags in the int representation\n",
        "        starting_id_vec = tf.tile(tf.constant([word_to_id['<GO>']], dtype=tf.int32), [batch_size], name='starting_id_vec')\n",
        "        \n",
        "        #using basic greedy to get next word in the inference time (based only on probs)\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embed_layer, \n",
        "                                                                    starting_id_vec, \n",
        "                                                                    word_to_id['<EOS>'])\n",
        "        \n",
        "        #Defining decoder - for inference time\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                            inference_helper, \n",
        "                                                            enc_states, \n",
        "                                                            output_layer)\n",
        "        \n",
        "        \n",
        "        inference_dec_output, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, \n",
        "                                                                       impute_finished=True, \n",
        "                                                                       maximum_iterations=max_seq_len)\n",
        "        \n",
        "    return train_dec_outputs, inference_dec_output\n",
        "\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def attention_mech(rnn_size, keep_probs, encoder_outputs, encoder_states, encoder_seq_len, batch_size):\n",
        "    \n",
        "\t\t#The helper function used to create attention mechanism in TF 1.4\n",
        "\n",
        "\t\t#Inputs:\n",
        "\t\t\t#rnn_size - number of units in the RNN layer\n",
        "\t\t\t#keep_probs -  dropout rate\n",
        "\t\t\t#encoder_outputs - ouputs got from the encoder part\n",
        "\t\t\t#encoder_states - states trained/got from encoder\n",
        "\t\t\t#encoder_seq_len - \n",
        "\t\t\t#batch_size - \n",
        "\n",
        "\t\t#Outputs:\n",
        "\t\t\t#dec_cell - attention based decoder cell\n",
        "\t\t\t#enc_state_new -new encoder stated with attention for the decoder\n",
        "\n",
        "\n",
        "    #using internal function to easier create RNN cell\n",
        "    def cell(units, probs):\n",
        "        layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
        "        return tf.contrib.rnn.DropoutWrapper(layer, probs)\n",
        "    \n",
        "    #defining rnn_cell\n",
        "    decoder_cell = cell(rnn_size, keep_probs)\n",
        "    \n",
        "    #using helper function from seq2seq sub_lib for Bahdanau attention\n",
        "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size, \n",
        "                                                               encoder_outputs, \n",
        "                                                               encoder_seq_len)\n",
        "    \n",
        "    #finishin attention with the attention holder - Attention Wrapper\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, \n",
        "                                                   attention_mechanism, \n",
        "                                                   rnn_size/2)\n",
        "    \n",
        "    #Here we are usingg zero_state of the LSTM (in this case) decoder cell, and feed the value of the last encoder_state to it\n",
        "    attention_zero = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
        "    enc_state_new = attention_zero.clone(cell_state=encoder_states[-1])\n",
        "    \n",
        "    return dec_cell, enc_state_new\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def opt_loss(outputs, targets, dec_seq_len, max_seq_len, learning_rate, clip_rate):\n",
        "    \n",
        "   \n",
        "    logits = tf.identity(outputs.rnn_output)\n",
        "    \n",
        "    mask_weigts = tf.sequence_mask(dec_seq_len, max_seq_len, dtype=tf.float32)\n",
        "    \n",
        "    with tf.variable_scope('opt_loss'):\n",
        "        #using sequence_loss to optimize the seq2seq model\n",
        "        loss = tf.contrib.seq2seq.sequence_loss(logits, \n",
        "                                                targets, \n",
        "                                                mask_weigts)\n",
        "        \n",
        "        #Define optimizer\n",
        "        opt = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        #Next 3 lines used to clip gradients {Prevent gradient explosion problem}\n",
        "        gradients = tf.gradients(loss, tf.trainable_variables())\n",
        "        clipped_grads, _ = tf.clip_by_global_norm(gradients, clip_rate)\n",
        "        traiend_opt = opt.apply_gradients(zip(clipped_grads, tf.trainable_variables()))\n",
        "        \n",
        "    return loss, traiend_opt\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "class Chatbot(object):\n",
        "    \n",
        "    def __init__(self, learning_rate, batch_size, enc_embed_size, dec_embed_size, rnn_size, \n",
        "                 number_of_layers, vocab_size, word_to_id, clip_rate):\n",
        "        \n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        self.inputs, self.targets, self.keep_probs, self.encoder_seq_len, self.decoder_seq_len, max_seq_len = grap_inputs()\n",
        "        \n",
        "        \n",
        "        enc_outputs, enc_states = encoders(self.inputs, \n",
        "                                          rnn_size,\n",
        "                                          number_of_layers, \n",
        "                                          self.encoder_seq_len, \n",
        "                                          self.keep_probs, \n",
        "                                          enc_embed_size, \n",
        "                                          vocab_size)\n",
        "        \n",
        "        dec_inputs = decoder_inputs_preprocessing(self.targets, \n",
        "                                                  word_to_id, \n",
        "                                                  batch_size)\n",
        "        \n",
        "        \n",
        "        decoder_cell, encoder_states_new = attention_mech(rnn_size, \n",
        "                                                          self.keep_probs, \n",
        "                                                          enc_outputs, \n",
        "                                                          enc_states, \n",
        "                                                          self.encoder_seq_len, \n",
        "                                                          batch_size)\n",
        "        \n",
        "        train_outputs, inference_output = decoders(dec_inputs, \n",
        "                                                  encoder_states_new, \n",
        "                                                  decoder_cell,\n",
        "                                                  dec_embed_size, \n",
        "                                                  vocab_size, \n",
        "                                                  self.decoder_seq_len, \n",
        "                                                  max_seq_len, \n",
        "                                                  word_to_id, \n",
        "                                                  batch_size)\n",
        "        \n",
        "        self.predictions  = tf.identity(inference_output.sample_id, name='preds')\n",
        "        \n",
        "        self.loss, self.opt = opt_loss(train_outputs, \n",
        "                                       self.targets, \n",
        "                                       self.decoder_seq_len, \n",
        "                                       max_seq_len, \n",
        "                                       learning_rate, \n",
        "                                       clip_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fX-ghPHWtsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "VOCAB_THRESHOLD = 5\n",
        "\n",
        "\n",
        "BUCKETS = [(10, 15), (15, 25), (25, 45), (45, 60), (60, 100)] #First try buckets you can tweak these\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "RNN_SIZE = 512\n",
        "\n",
        "NUM_LAYERS = 3\n",
        "\n",
        "ENCODING_EMBED_SIZE = 512\n",
        "DECODING_EMBED_SIZE = 512\n",
        "\n",
        "LEARNING_RATE = 0.0001\n",
        "LEARNING_RATE_DECAY = 0.9 #nisam siguran da cu ovo koristiti\n",
        "MIN_LEARNING_RATE = 0.0001\n",
        "\n",
        "KEEP_PROBS = 0.5\n",
        "\n",
        "CLIP_RATE = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AUft6EgoWiK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def get_conversations():\n",
        "\t\n",
        "\t\t\n",
        "\t\t#Function made ONLY for Cornell dataset to extract conversations from the raw file.\n",
        "\n",
        "\t\n",
        "\tconversations = []\n",
        "\twith open('raw_cornell_data/movie_conversations.txt', 'r') as f:\n",
        "\t\tfor line in f.readlines():\n",
        "\t\t\t\n",
        "\t\t\tconversation = line.split(' +++$+++ ')[-1]\n",
        "\t\t\tconversation = conversation.replace(\"'\", \"\")\n",
        "\t\t\tconversation = conversation[1:-2]\n",
        "\t\t\tconversation = conversation.split(\", \")\n",
        "\t\t\tconversations.append(conversation)\n",
        "\n",
        "\treturn conversations\n",
        "\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def get_movie_lines():\n",
        "\n",
        "\t\n",
        "\t\t#The helper function used to extract movie_lines from the Cornell dataset\n",
        "\n",
        "\t\n",
        "\tsentences = {}\n",
        "\twith open('raw_cornell_data/movie_lines.txt', 'r') as f:\n",
        "\t\tfor line in f.readlines():\n",
        "\t\t\tsentences[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1].replace('\\n', \"\")\n",
        "\n",
        "\treturn sentences\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def questions_vs_answers(convs, lines):\n",
        "\t\n",
        "\n",
        "\t\t#Save to the file questions and answers extracted from the raw files. VERSION 1\n",
        "\n",
        "\t\n",
        "\n",
        "\tfor i in range(len(convs)):\n",
        "\t\tconversation = convs[i]\n",
        "\t\tif len(conversation) % 2 == 0:\n",
        "\t\t\tfor line in range(len(conversation)):\n",
        "\t\t\t\tif line % 2 == 0:\n",
        "\t\t\t\t\twith open('movie_questions.txt', 'a') as f:\n",
        "\t\t\t\t\t\tf.write(lines[conversation[line]] + \"\\n\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\twith open('movie_answers.txt', 'a') as f:\n",
        "\t\t\t\t\t\tf.write(lines[conversation[line]] + \"\\n\")\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def questions_vs_answers_v2(convs, lines):\n",
        "\n",
        "\n",
        "\t\t#Save to the file questions and answers extracted from the raw files. VERSION 2\n",
        "\n",
        "\n",
        "\tfor i in range(len(convs)):\n",
        "\t\tconversation = convs[i]\n",
        "\t\tfor line in range(len(conversation) - 1):\n",
        "\n",
        "\t\t\twith open('movie_questions_2.txt', 'a') as f:\n",
        "\t\t\t\tf.write(lines[conversation[line]] + \"\\n\")\n",
        "\t\t\twith open('movie_answers_2.txt', 'a') as f:\n",
        "\t\t\t\tf.write(lines[conversation[line + 1]] + \"\\n\")\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def cornell_tokenizer(text):\n",
        "\t\n",
        "\t\t#Basic, starting tokenizer used for sentence preprocessing.\n",
        "\n",
        "\t\n",
        "\ttext = re.sub(r\"\\'m\", \" am\", text)\n",
        "\ttext = re.sub(r\"\\'s\", \" is\", text)\n",
        "\ttext = re.sub(r\"\\'re\", \" are\", text)\n",
        "\ttext = re.sub(r\"\\'ll\", \" will\", text)\n",
        "\ttext = re.sub(r\"\\'d\", \" would\", text)\n",
        "\ttext = re.sub(r\"won't\", \"will not\", text)\n",
        "\ttext = re.sub(r\"can't\", \"cannot\", text)\n",
        "\ttext = re.sub(r\"\\.\", \" . \", text)\n",
        "\ttext = re.sub(r\"\\?\", \" ? \", text)\n",
        "\ttext = re.sub(r\"!\", \" ! \", text)\n",
        "\ttext = re.sub(r\"/\", \" / \", text)\n",
        "\ttext = re.sub(r\",\", \" , \", text)\n",
        "\ttext = re.sub(r'\"', ' \" ', text)\n",
        "\ttext = re.sub(r\"-\", \" - \", text)\n",
        "\n",
        "\ttext = re.sub(r\"[-<>{}+=|?'()\\:@]\", \"\", text)\n",
        "\treturn text.replace('\\n', '')\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def clean_data():\n",
        "\t\n",
        "\t\t#Raw data clearner.\n",
        "\t\n",
        "\tcleaned_questions = []\n",
        "\tcleaned_answers = []\n",
        "  #cnt=0\n",
        "\twith codecs.open('gdrive/My Drive/movie_questions_2.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
        "    #str = unicode(str, errors='ignore')\n",
        "    #a.encode('utf-8').strip()\n",
        "\t\tlines = f.readlines()\n",
        "    \n",
        "\t\tfor line in lines:\n",
        "\t\t\tcleaned_questions.append(cornell_tokenizer(line))\n",
        "\n",
        "\twith codecs.open('gdrive/My Drive/movie_answers_2.txt', 'r', encoding = 'utf-8', errors = 'ignore') as f:\n",
        "\t\tlines = f.readlines()\n",
        " \n",
        "\t\tfor line in lines:\n",
        "     \n",
        "\t\t\tcleaned_answers.append(cornell_tokenizer(line))\n",
        "      \n",
        "\treturn cleaned_questions, cleaned_answers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def create_vocab(questions, answers):\n",
        "\n",
        "\t\n",
        "\t\t\n",
        "\t\t#This function is used to create vocabulary, word_to_id and id_to_word dicts from cleaned data (got from the last question).\n",
        "\n",
        "\n",
        "\tassert len(questions) == len(answers)\n",
        "\tvocab = []\n",
        "\tfor i in range(len(questions)):\n",
        "\t\twords = questions[i].split()\n",
        "\t\tfor word in words:\n",
        "\t\t\tvocab.append(word)\n",
        "\n",
        "\t\twords = answers[i].split()\n",
        "\t\tfor word in words:\n",
        "\t\t\tvocab.append(word)\n",
        "\n",
        "\n",
        "\tvocab = Counter(vocab)\n",
        "\tnew_vocab = []\n",
        "\tfor key in vocab.keys():\n",
        "\t\tif vocab[key] >= VOCAB_THRESHOLD:\n",
        "\t\t\tnew_vocab.append(key)\n",
        "\n",
        "\tnew_vocab = ['<PAD>', '<GO>', '<UNK>', '<EOS>'] + new_vocab\n",
        "\n",
        "\tword_to_id = {word:i for i, word in enumerate(new_vocab)}\n",
        "\tid_to_word = {i:word for i, word in enumerate(new_vocab)}\n",
        "\n",
        "\treturn new_vocab, word_to_id, id_to_word\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def encoder(data, word_to_id, targets=False):\n",
        "\t\n",
        "\t\t#Using word_to_id dictionery to map each word in the sample to it's own int representation\n",
        "\n",
        "\t\n",
        "\tencoded_data = []\n",
        "\n",
        "\tfor i in range(len(data)):\n",
        "\n",
        "\t\tencoded_line = []\n",
        "\t\twords = data[i].split()\n",
        "\t\tfor word in words:\n",
        "\n",
        "\t\t\tif word not in word_to_id.keys():\n",
        "\t\t\t\tencoded_line.append(word_to_id['<UNK>'])\n",
        "\t\t\telse:\n",
        "\t\t\t\tencoded_line.append(word_to_id[word])\n",
        "\n",
        "\t\tif targets:\n",
        "\t\t\tencoded_line.append(word_to_id['<EOS>'])\n",
        "\n",
        "\t\tencoded_data.append(encoded_line)\n",
        "\n",
        "          \n",
        "\treturn np.array(encoded_data)\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def pad_data(data, word_to_id, max_len, target=False):\n",
        "\t\t#If the sentence is shorter then wanted length, pad it to that length\n",
        "\n",
        "\tif target:\n",
        "\t\treturn data + [word_to_id['<PAD>']] * (max_len - len(data))\n",
        "\telse:\n",
        "\t\treturn [word_to_id['<PAD>']] * (max_len - len(data)) + data\n",
        "\n",
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def bucket_data(questions, answers, word_to_id):\n",
        "\n",
        "\t\n",
        "\t\t#If you prefere bucketing version of the padding, use this function to create buckets of your data.\n",
        "\n",
        "\tassert len(questions) == len(answers)\n",
        "\n",
        "\tbucketed_data = []\n",
        "\talready_added = []\n",
        "\tfor bucket in BUCKETS:\n",
        "\t\tdata_for_bucket = []\n",
        "\t\tencoder_max = bucket[0]\n",
        "\t\tdecoder_max = bucket[1]\n",
        "\t\tfor i in range(len(questions)):\n",
        "\t\t\tif len(questions[i]) <= encoder_max and len(answers[i]) <= decoder_max:\n",
        "\t\t\t\tif i not in already_added:\n",
        "\t\t\t\t\tdata_for_bucket.append((pad_data(questions[i], word_to_id, encoder_max), pad_data(answers[i], word_to_id, decoder_max, True)))\n",
        "\t\t\t\t\talready_added.append(i)\n",
        "\n",
        "\t\tbucketed_data.append(data_for_bucket)\n",
        "    #print(bucketed_data)\n",
        "    #print(bucketed_data)\n",
        "\n",
        "\treturn bucketed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZY5KkGlTLGW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define get_accuracy helper function to check accuracy of the sequence data"
      ]
    },
    {
      "metadata": {
        "id": "WpuMIsoMTLGZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ikj098gjL0c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def int2str(strings):\n",
        "    answer = ''\n",
        "    for i in strings:\n",
        "        if id_to_word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif id_to_word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif id_to_word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + id_to_word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsxs-a_MTLGl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "metadata": {
        "id": "oby9BM6ETLGo",
        "colab_type": "code",
        "outputId": "c04fe104-ba9e-4ea3-d26c-eee71e7c1581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "cleaned_questions, cleaned_answers = clean_data()\n",
        "print(cleaned_questions[0:3])\n",
        "print(cleaned_answers[0:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Can we make this quick    Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break   up on the quad .   Again . ', 'Well ,  I thought we would start with pronunciation ,  if that is okay with you . ', 'Not the hacking and gagging and spitting part .   Please . ']\n",
            "['Well ,  I thought we would start with pronunciation ,  if that is okay with you . ', 'Not the hacking and gagging and spitting part .   Please . ', 'Okay .  .  .  then how bout we try out some French cuisine .   Saturday    Night  ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2G2NuulCTLG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating vocab and necessary dictionaries"
      ]
    },
    {
      "metadata": {
        "id": "T56e3BIWTLG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "vocab, word_to_id, id_to_word = create_vocab(cleaned_questions, cleaned_answers)\n",
        "#print(id_to_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mpzOLuDTLHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data encoding"
      ]
    },
    {
      "metadata": {
        "id": "bR0mjHmHTLHN",
        "colab_type": "code",
        "outputId": "48ce1641-31e9-43af-cbbd-cf38d3a38b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "encoded_questions = encoder(cleaned_questions, word_to_id)\n",
        "print(encoded_questions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([4, 5, 6, 7, 8, 2, 2, 9, 10, 11, 12, 13, 14, 15, 2, 16, 17, 18, 19, 20, 2, 21, 22, 21])\n",
            " list([23, 24, 25, 26, 5, 27, 28, 29, 2, 24, 30, 31, 32, 33, 29, 34, 21])\n",
            " list([35, 20, 36, 9, 37, 9, 38, 39, 21, 40, 21]) ...\n",
            " list([3452, 24874, 670, 102, 136, 54, 102, 1474, 248, 29, 66, 2, 21])\n",
            " list([23, 25, 2632, 34, 24, 1701, 24, 25, 69, 65, 6515, 102, 1637, 13693, 21, 2256])\n",
            " list([192, 25, 2632, 34, 24, 34, 126, 208, 461, 2582, 25, 27, 103, 9308, 133, 59, 1582, 436, 21, 58, 69, 59, 11073, 618])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M_O4ep2WTLHX",
        "colab_type": "code",
        "outputId": "03ab67e2-1f7d-4dbc-d553-4b7615d91f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "encoded_answers = encoder(cleaned_answers, word_to_id, True)\n",
        "print(encoded_answers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([23, 24, 25, 26, 5, 27, 28, 29, 2, 24, 30, 31, 32, 33, 29, 34, 21, 3])\n",
            " list([35, 20, 36, 9, 37, 9, 38, 39, 21, 40, 21, 3])\n",
            " list([41, 21, 21, 21, 42, 43, 44, 5, 45, 46, 47, 48, 49, 21, 50, 51, 3])\n",
            " ...\n",
            " list([25, 168, 24874, 1051, 70, 144, 679, 19, 20, 5168, 97, 300, 6274, 70, 2, 4247, 9, 8652, 70, 6934, 4550, 152, 6935, 21, 3])\n",
            " list([192, 25, 2632, 34, 24, 34, 126, 208, 461, 2582, 25, 27, 103, 9308, 133, 59, 1582, 436, 21, 58, 69, 59, 11073, 618, 3])\n",
            " list([220, 920, 118, 638, 11073, 21, 203, 5, 69, 154, 2767, 79, 70, 530, 2, 9237, 4497, 24, 431, 167, 21, 3])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "inkokQC5TAX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TESTING DECODER"
      ]
    },
    {
      "metadata": {
        "id": "zlo5t1PlS533",
        "colab_type": "code",
        "outputId": "72932992-6a12-406e-b60a-10c5e0e72d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def convert_string2int(question, word2int):\n",
        "    question = cornell_tokenizer(question)\n",
        "    return [word2int.get(word, word2int['<UNK>']) for word in question.split()]\n",
        "quest = cleaned_questions[0]\n",
        "question = convert_string2int(quest, word_to_id)\n",
        "print(question)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 5, 6, 7, 8, 2, 2, 9, 10, 11, 12, 13, 14, 15, 2, 16, 17, 18, 19, 20, 2, 21, 22, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wN3BUegOUuJH",
        "colab_type": "code",
        "outputId": "c5edd959-d203-43f2-bb61-a175329c3303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "def convert_int2string(answer, int2word):\n",
        "    #question = cornell_tokenizer(question)\n",
        "    answer = \" \".join([int2word.get(word, '<UNK>') for word in answer])\n",
        "    return answer\n",
        "#quest = cleaned_answers[0]\n",
        "answer = convert_int2string(question, id_to_word)\n",
        "#question = convert_string2int(quest, word_to_id)\n",
        "print(answer)\n",
        "#print(int2word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick <UNK> <UNK> and Andrew Barrett are having an incredibly <UNK> public break up on the <UNK> . Again .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JHmAlY6aTLHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bucketting data"
      ]
    },
    {
      "metadata": {
        "id": "o_bqFCs9TLHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "bucketed_data = bucket_data(encoded_questions, encoded_answers, word_to_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UDxCtxHNoq4W",
        "colab_type": "code",
        "outputId": "0cb786ac-9b0c-4fa9-ab88-aa236059f1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "print(bucketed_data[0][0][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[130, 21, 131, 52, 12, 132, 133, 20, 134, 21, 3, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IToR1i_XTLH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating model object, session and defining model saver"
      ]
    },
    {
      "metadata": {
        "id": "T1fpmFygTLH5",
        "colab_type": "code",
        "outputId": "ec9910fc-4af2-4bc7-df1e-444f264bff0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "model = Chatbot(LEARNING_RATE, \n",
        "                BATCH_SIZE, \n",
        "                ENCODING_EMBED_SIZE, \n",
        "                DECODING_EMBED_SIZE, \n",
        "                RNN_SIZE, \n",
        "                NUM_LAYERS,\n",
        "                len(vocab), \n",
        "                word_to_id, \n",
        "                CLIP_RATE) #4=clip_rate "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-c0ce43bfef97>:64: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pkNR2nTFTLID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAr7f6WoTLIM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMNvNnFBTLIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver(max_to_keep=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0TUR74uTLIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Entering big buckets, training loop"
      ]
    },
    {
      "metadata": {
        "id": "OdQv26HdlLSB",
        "colab_type": "code",
        "outputId": "c07f0972-2410-4ca3-f438-1bb3bd702388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "import os\n",
        "\n",
        "check=os.listdir(\"gdrive/My Drive/checkpoint\")\n",
        "loadID=len(check)-1\n",
        "print(loadID)\n",
        "#check_points = listdir(\"gdrive/My Drive/checkpoint1/epoch{}/chatbot.ckpt\".format(loadID))\n",
        "#checkpt_loadID = len(check_points) - 1\n",
        "#barbie = os.listdir(\"gdrive/My Drive/checkpoint1/epoch0/chatbot.ckpt\")\n",
        "#saver.restore(session, barbie)\n",
        "if(loadID>-1):\n",
        "  #saver.restore(session, \"gdrive/My Drive/checkpoint1/epoch{}/chatbot.ckpt\".format(0))\n",
        "  check=os.listdir(\"gdrive/My Drive/checkpoint/epoch{}\".format(loadID))\n",
        "  BucketID=((len(check)-1)//3)-1\n",
        "  if(BucketID>-1):\n",
        "    saver.restore(session, \"gdrive/My Drive/checkpoint/epoch{}/chatbot_{}.ckpt\".format(loadID,BucketID))\n",
        "    BucketID=BucketID+1\n",
        "  else:\n",
        "    print(\"There is no checkpoint\")\n",
        "    loadID=0\n",
        "    BucketID=0\n",
        "  #for i in range (0, num_checkpts):\n",
        "    \n",
        " #   saver.restore(session,\"gdrive/My Drive/checkpoint1/epoch{}/chatbot_{}.ckpt\".format(loadID,i))\n",
        "  #saver.restore(session, \"gdrive/My Drive/checkpoint1/epoch{}/chatbot.ckpt\".format(loadID))\n",
        "else:\n",
        "  print(\"There is no checkpoint\")\n",
        "  loadID=0\n",
        "  BucketID=0\n",
        "  \n",
        "  #saver.restore(session, \"gdrive/My Drive/checkpoint1/epoch{}/checkpoint\".format(loadID)) ### only allowed to restore .ckpt files\n",
        "print(loadID,BucketID)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99\n",
            "INFO:tensorflow:Restoring parameters from gdrive/My Drive/checkpoint/epoch99/chatbot_4.ckpt\n",
            "99 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wAs0-p0Ys0eZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  #check=os.listdir(\"gdrive/My Drive/test\")\n",
        "  #print(check)\n",
        "  #saver.restore(session, \"gdrive/My Drive/test/checkpoint\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DITMaE4780J2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ZjkIc5gCTLIi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "for i in range(loadID,EPOCHS):\n",
        "    epoch_accuracy = []\n",
        "    epoch_loss = []\n",
        "    for b in range(BucketID,len(bucketed_data)):\n",
        "        bucket = bucketed_data[b]\n",
        "        questions_bucket = []\n",
        "        answers_bucket = []\n",
        "        bucket_accuracy = []\n",
        "        bucket_loss = []\n",
        "        for k in range(len(bucket)):\n",
        "            questions_bucket.append(np.array(bucket[k][0]))\n",
        "            answers_bucket.append(np.array(bucket[k][1]))\n",
        "        #for ii in tqdm(range(len(questions_bucket) //  BATCH_SIZE)):\n",
        "        Number_of_Loop = len(questions_bucket) / BATCH_SIZE\n",
        "        Number_of_Loop = math.trunc(Number_of_Loop)\n",
        "        for ii in tqdm(range(Number_of_Loop)):\n",
        "            \n",
        "        #for ii in tqdm(range(len(questions_bucket) //  BATCH_SIZE)):\n",
        "            \n",
        "            starting_id = ii * BATCH_SIZE\n",
        "            \n",
        "            X_batch = questions_bucket[starting_id:starting_id+BATCH_SIZE]\n",
        "            y_batch = answers_bucket[starting_id:starting_id+BATCH_SIZE]\n",
        "            \n",
        "            feed_dict = {model.inputs:X_batch, \n",
        "                         model.targets:y_batch, \n",
        "                         model.keep_probs:KEEP_PROBS, \n",
        "                         model.decoder_seq_len:[len(y_batch[0])]*BATCH_SIZE,\n",
        "                         model.encoder_seq_len:[len(X_batch[0])]*BATCH_SIZE}\n",
        "            \n",
        "            cost, _, preds = session.run([model.loss, model.opt, model.predictions], feed_dict=feed_dict)\n",
        "            \n",
        "            epoch_accuracy.append(get_accuracy(np.array(y_batch), np.array(preds)))\n",
        "            bucket_accuracy.append(get_accuracy(np.array(y_batch), np.array(preds)))\n",
        "            \n",
        "            bucket_loss.append(cost)\n",
        "            epoch_loss.append(cost)\n",
        "            #for s in preds:\n",
        "              #print(\"Chatbot: \",int2str(s))\n",
        "        saver.save(session, \"gdrive/My Drive/checkpoint/epoch{}/chatbot_{}.ckpt\".format(i,b))    \n",
        "        print(\"Bucket {}:\".format(b+1), \n",
        "              \" | Loss: {}\".format(np.mean(bucket_loss)), \n",
        "              \" | Accuracy: {}\".format(np.mean(bucket_accuracy)))\n",
        "    BucketID=0\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPULdDWuo_mR",
        "colab_type": "code",
        "outputId": "2c0bdc86-da98-4414-9299-612940945862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        }
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "for b in range(len(bucketed_data)):\n",
        "    bucket = bucketed_data[b]\n",
        "    print(len(bucket))\n",
        "    for l  in range (10):\n",
        "        print(l)\n",
        "        predicted_question=bucket[l][0]\n",
        "        question = ''\n",
        "        for i in predicted_question:\n",
        "            if id_to_word[i] == 'i':\n",
        "                token = ' I'\n",
        "            elif id_to_word[i] == '<EOS>':\n",
        "                token = '.'\n",
        "            elif id_to_word[i] == '<OUT>':\n",
        "                token = 'out'\n",
        "            else:\n",
        "                token = ' ' + id_to_word[i]\n",
        "            question += token\n",
        "            if token == '.':\n",
        "                break\n",
        "        print(question)\n",
        "        predicted_answer=bucket[l][1]\n",
        "        answer = ''\n",
        "        for i in predicted_answer:\n",
        "            if id_to_word[i] == 'i':\n",
        "                token = ' I'\n",
        "            elif id_to_word[i] == '<EOS>':\n",
        "                token = '.'\n",
        "            elif id_to_word[i] == '<OUT>':\n",
        "                token = 'out'\n",
        "            else:\n",
        "                token = ' ' + id_to_word[i]\n",
        "            answer += token\n",
        "            if token == '.':\n",
        "                break\n",
        "        print(answer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91537\n",
            "0\n",
            " <PAD> <PAD> <UNK> ma <UNK> . This is my head\n",
            " Right . See You are ready for the quiz ..\n",
            "1\n",
            " That is because it is such a nice one .\n",
            " Forget French ..\n",
            "2\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> There .\n",
            " Where.\n",
            "3\n",
            " <PAD> <PAD> You have my word . As a gentleman\n",
            " You are sweet ..\n",
            "4\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Hi .\n",
            " Looks like things worked out tonight , huh.\n",
            "5\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> You know <UNK>\n",
            " I believe we share an art instructor.\n",
            "6\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Have fun tonight\n",
            " Tons.\n",
            "7\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I was\n",
            " You never wanted to go out with <UNK> , did you.\n",
            "8\n",
            " <PAD> <PAD> <PAD> <PAD> Well , no . . .\n",
            " Then that is all you had to say ..\n",
            "9\n",
            " <PAD> Then that is all you had to say .\n",
            " But.\n",
            "49872\n",
            "0\n",
            " <PAD> <PAD> <PAD> <PAD> Not the hacking and gagging and spitting part . Please .\n",
            " Okay . . . then how bout we try out some French cuisine . Saturday Night.\n",
            "1\n",
            " <PAD> No , no , it is my fault we didnt have a proper introduction\n",
            " Cameron ..\n",
            "2\n",
            " <PAD> <PAD> Gosh , if only we could find Kat a boyfriend . . .\n",
            " Let me see what I can do ..\n",
            "3\n",
            " <PAD> <PAD> <PAD> <PAD> How is our little Find the <UNK> A Date plan <UNK>\n",
            " Well , there is someone I think might be.\n",
            "4\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> You got something on your mind\n",
            " I counted on you to help my cause . You and that thug are obviously failing . Arent we ever going on our date.\n",
            "5\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> How do you get your hair to look like that\n",
            " <UNK> is Deep <UNK> every two days . And I never , ever use a <UNK> without the <UNK> attachment ..\n",
            "6\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Sure have .\n",
            " I really , really , really wanna go , but I cannot . Not unless my sister goes ..\n",
            "7\n",
            " <PAD> <PAD> <PAD> <PAD> So that is the kind of guy she likes Pretty ones\n",
            " Who knows All Ive ever heard her say is that she would dip before dating a guy that smokes ..\n",
            "8\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> What crap\n",
            " Me . This endless . . . blonde babble . I am like , boring myself ..\n",
            "9\n",
            " <PAD> <PAD> <PAD> <PAD> I figured you would get to the good stuff eventually .\n",
            " What good stuff.\n",
            "47319\n",
            "0\n",
            " <PAD> Can we make this quick <UNK> <UNK> and Andrew Barrett are having an incredibly <UNK> public break up on the <UNK> . Again .\n",
            " Well , I thought we would start with <UNK> , if that is okay with you ..\n",
            "1\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Well , I thought we would start with <UNK> , if that is okay with you .\n",
            " Not the hacking and gagging and spitting part . Please ..\n",
            "2\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> You are asking me out . That is so cute . What is your name again\n",
            " Forget it ..\n",
            "3\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Cameron .\n",
            " The thing is , Cameron I am at the mercy of a particularly hideous breed of loser . My sister . I cannot date until she does ..\n",
            "4\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Why\n",
            " Unsolved mystery . She used to be really popular when she started high school , then it was just like she got sick of it or something ..\n",
            "5\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I really , really , really wanna go , but I cannot . Not unless my sister goes .\n",
            " I am workin on it . But she doesnt seem to be goin for him ..\n",
            "6\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> She is not a . . .\n",
            " <UNK> No . I found a picture of <UNK> Leto in one of her drawers , so I am pretty sure she is not harboring same sex tendencies ..\n",
            "7\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I looked for you back at the party , but you always seemed to be \" occupied \" .\n",
            " I was.\n",
            "8\n",
            " <PAD> <PAD> <PAD> Then <UNK> says , \" If you go any lighter , you are gonna look like an extra on <UNK> . \"\n",
            " No . . ..\n",
            "9\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Me . This endless . . . blonde babble . I am like , boring myself .\n",
            " Thank God ! If I had to hear one more story about your <UNK> . . ..\n",
            "22622\n",
            "0\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> The thing is , Cameron I am at the mercy of a particularly hideous breed of loser . My sister . I cannot date until she does .\n",
            " Seems like she could get a date easy enough . . ..\n",
            "1\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Unsolved mystery . She used to be really popular when she started high school , then it was just like she got sick of it or something .\n",
            " That is a shame ..\n",
            "2\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Right . See You are ready for the quiz .\n",
            " I dont want to know how to say that though . I want to know useful things . Like where the good stores are . How much does champagne cost Stuff like <UNK> . I have never in my life had to point out my head to someone ..\n",
            "3\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <UNK> No . I found a picture of <UNK> Leto in one of her drawers , so I am pretty sure she is not harboring same sex tendencies .\n",
            " So that is the kind of guy she likes Pretty ones.\n",
            "4\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> He practically proposed when he found out we had the same <UNK> . I mean . Dr . <UNK> is great an all , but he is not exactly relevant party conversation .\n",
            " Is he oily or dry.\n",
            "5\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Sometimes I wonder if the guys we are supposed to want to go out with are the ones we actually want to go out with , you know\n",
            " All I know is I would give up my private line to go out with a guy like Joey ..\n",
            "6\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> So yeah , Ive got the Sears catalog thing going and the tube sock gig \" that is gonna be huge . And then I am up for an ad for Queen Harry next week .\n",
            " Queen Harry.\n",
            "7\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Oh , I thought you might have a date I dont know why I am bothering to ask , but are you going to Bogey Lowenstein is party Saturday night\n",
            " What do you think.\n",
            "8\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I dont get you . You act like you are too good for any of this , and then you go totally <UNK> when you get here .\n",
            " You are welcome ..\n",
            "9\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> After that , I swore I would never do anything just because \" everyone else \" was doing it . And I havent since . Except for Bogey is party , and my stunning <UNK> intestinal display\n",
            " Why didnt you tell me.\n",
            "6593\n",
            "0\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I dont want to know how to say that though . I want to know useful things . Like where the good stores are . How much does champagne cost Stuff like <UNK> . I have never in my life had to point out my head to someone .\n",
            " That is because it is such a nice one ..\n",
            "1\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> I realize that the men of this fine institution are severely lacking , but killing yourself so you can be with William Shakespeare is beyond the scope of normal teenage obsessions . You are <UNK> far past daytime talk show <UNK> and entering the world of those who need very expensive therapy .\n",
            " But imagine the things he would say during sex ..\n",
            "2\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> What\n",
            " That is where I was last year . She would never lived alone my grandfather died I stayed with her . I wasnt in jail , I dont know Marilyn Manson , and Ive never slept with a Spice Girl . I spent a year sitting next to my grandma on the couch watching Wheel of Fortune . End of story ..\n",
            "3\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> End my career\n",
            " How are you going to fight this Maybe if Oleg hadnt gotten away and you would been on the front page , as a hero , this thing would be easier to fight . You would have the good to weight against the bad ! It is unfortunate that I have to make decisions based upon your press coverage but there is nothing I can do ! Gimme your shield ..\n",
            "4\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Maybe it is a ritual thing or someone trying to send a message . <UNK> rites are taken very seriously in Eastern Europe . It could be to humiliate them . Just burning them up , no proper funeral , it is like <UNK> them to hell .\n",
            " Eastern Europe . Like what <UNK> Hungary.\n",
            "5\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Yeah , just her in the shower . Nothing happened . Look , I am sure you probably think I am a fool and I fucked up , but . . .\n",
            " No , I dont think you were a fool , I just think you were stupid about it . I mean , to say the least , you outta know better . You dont know her well enough . She is got the potential to fucking hang you even if she suggests that you made a pass at her , it is fuckin over . You can deny it all you want , but it will not make one fucking bit of difference . You are dead ..\n",
            "6\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> How old are your kids\n",
            " My kids Let is see . . . Susan is 15 . <UNK> is 9 . Dont tell me you are thinking about having a kid ! How old are you Never mind . Let me just tell you this Every stupid cliche you hear about kids they change your life , they make you a better person , they make you whole . . . It is all true ! Before I had kids when friends talked about their kids , I wanted to vomit . Now I get it . Am I right , Leon.\n",
            "7\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Hey , tabloids paid Ted Bundy famous serial killer half a million for his interview . And how much you think Monica got for writing book about the President coming on to her It pays to be a killer or a whore in this country . Look , you want magazine or not\n",
            " Yes . Both ..\n",
            "8\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> Movies . . . to be in the movies or to see movies\n",
            " Yes . No . Both . When I was a boy , I see movie at school called \" It is a Wonderful Life \" directed by Frank Capra . Ever since I want to come to America . Land of the free . Home of the brave . A land where anyone can be anything . As long as they are white ..\n",
            "9\n",
            " <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> That is right . Well , naturally , I didnt say anything to <UNK> . I assumed they would start paying me at the higher grade on the next pay cheque . But it is been almost three weeks now and I am still being paid as an <UNK> 18 .\n",
            " Interesting that you mention it , because Ive got the same problem ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TfTnzYlXiMMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Adapted from: https://github.com/AbrahamSanders/seq2seq-chatbot and Based on https://github.com/lucko515/chatbot-startkit; Retrieved on 10-04-2018\n",
        "########## PART 4 - TESTING THE SEQ2SEQ MODEL ##########\n",
        " \n",
        "#print(word_to_id)\n",
        "#cb = Chatbot()\n",
        " \n",
        "# Loading the weights and Running the session\n",
        "#checkpoint = \"./chatbot_weights.ckpt\"\n",
        "#checkpoint = os.listdir(\"gdrive/My Drive/checkpoint/epoch0/chatbot.ckpt\")\n",
        "'''\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()\n",
        "#saver.restore(session, checkpoint)\n",
        " '''\n",
        "# Converting the questions from strings to lists of encoding integers\n",
        "def convert_string2int(question, word2int):\n",
        "    question = cornell_tokenizer(question)\n",
        "    return [word2int.get(word, word2int['<UNK>']) for word in question.split()]\n",
        " \n",
        "# Setting up the chat\n",
        "while(True):\n",
        "    question = input(\"You: \")\n",
        "    if question == 'Goodbye':\n",
        "      break\n",
        "    inBucket=0\n",
        "    outBucket=0\n",
        "    question = convert_string2int(question, word_to_id)\n",
        "    question = question + [word_to_id['<EOS>']]\n",
        "    if(len(question)<=10):\n",
        "        inBucket=10\n",
        "        outBucket=15\n",
        "    elif(len(question)<=15):\n",
        "        inBucket=15\n",
        "        outBucket=25\n",
        "    elif(len(question)<=25):\n",
        "        inBucket=25\n",
        "        outBucket=45\n",
        "    elif(len(question)<=45):\n",
        "        inBucket=45\n",
        "        outBucket=60\n",
        "    elif(len(question)<=60):\n",
        "        inBucket=60\n",
        "        outBucket=100\n",
        "    question =  [word_to_id['<PAD>']] * (inBucket - len(question)) + question\n",
        "    \n",
        "    #print(question)\n",
        "    #print(outBucket)\n",
        "    #print(inBucket)\n",
        "    #print(BATCH_SIZE)\n",
        "    #fake_batch = np.zeros((BATCH_SIZE, inBucket))\n",
        "    #fake_batch[0] = question\n",
        "    fake_batch=[question]*BATCH_SIZE\n",
        "    predicted_answer = session.run(model.predictions, {model.inputs: fake_batch, model.keep_probs: 0.5,\n",
        "                                  model.decoder_seq_len:[outBucket]*BATCH_SIZE,\n",
        "                                  model.encoder_seq_len:[inBucket]*BATCH_SIZE})\n",
        "    answer = ''\n",
        "    #print(predicted_answer)\n",
        "    for i in predicted_answer[0]:\n",
        "        if id_to_word[i] == 'i':\n",
        "            token = ' I'\n",
        "        elif id_to_word[i] == '<EOS>':\n",
        "            token = '.'\n",
        "        elif id_to_word[i] == '<OUT>':\n",
        "            token = 'out'\n",
        "        else:\n",
        "            token = ' ' + id_to_word[i]\n",
        "        answer += token\n",
        "        if token == '.':\n",
        "            break\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}